# Logstash Configuration for Acey Backend
# Processing logs from multiple sources into Elasticsearch

input {
  # Application logs from file
  file {
    path => "/var/log/acey/app.log"
    start_position => "beginning"
    sincedb_path => "/var/lib/logstash/sincedb_app"
    codec => "json"
    tags => ["application", "acey"]
  }
  
  # Nginx access logs
  file {
    path => "/var/log/nginx/access.log"
    start_position => "beginning"
    sincedb_path => "/var/lib/logstash/sincedb_nginx"
    codec => "json"
    tags => ["nginx", "access"]
  }
  
  # System logs
  file {
    path => "/var/log/syslog"
    start_position => "beginning"
    sincedb_path => "/var/lib/logstash/sincedb_syslog"
    tags => ["system"]
  }
  
  # Docker container logs
  docker {
    path => "/var/lib/docker/containers"
    start_position => "beginning"
    tags => ["docker"]
  }
}

filter {
  # Parse JSON logs
  if [tags][0] == "application" {
    json {
      source => "message"
    }
    
    # Extract timestamp
    date {
      match => [ "timestamp", "ISO8601" ]
    }
    
    # Add environment
    mutate {
      add_field => { "environment" => "%{ENVIRONMENT:production}" }
    }
  }
  
  # Parse Nginx logs
  if [tags][0] == "nginx" {
    grok {
      match => { 
        "message" => "%{NGINXACCESS}"
      }
    }
    
    # Convert response time to float
    mutate {
      convert => { "response_time" => "float" }
    }
    
    # Add geoip data
    geoip {
      source => "client_ip"
    }
  }
  
  # Add service information
  mutate {
    add_field => { "service" => "acey-backend" }
    add_field => { "version" => "%{APP_VERSION:1.0.0}" }
  }
  
  # Remove sensitive data
  if [message] =~ /password|token|secret|key/ {
    mutate {
      replace => { "message" => "REDACTED_SENSITIVE_DATA" }
    }
  }
  
  # Add computed fields
  if [response_time] {
    ruby {
      code => "event.set('response_time_ms', event.get('response_time') * 1000)"
    }
  }
}

output {
  # Send to Elasticsearch
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "acey-logs-%{+YYYY.MM.dd}"
    template_name => "acey-logs"
    template_pattern => "acey-logs-*"
    template_overwrite => true
    template => "/etc/logstash/templates/acey-logs-template.json"
    
    # Performance settings
    flush_size => 500
    idle_flush_time => 1
    
    # Retry settings
    retry_on_conflict => true
    retry_max_items => 500
    retry_max_interval => 5
  }
  
  # Debug output (remove in production)
  if [environment] == "development" {
    stdout {
      codec => rubydebug
    }
  }
  
  # Error handling
  if "_grokparsefailure" in [tags] {
    file {
      path => "/var/log/logstash/grok_failures.log"
    }
  }
}
