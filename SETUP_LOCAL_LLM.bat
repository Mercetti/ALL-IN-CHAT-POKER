@echo off
setlocal enabledelayedexpansion

echo ğŸ¤– Setting up Helm Control with Local LLM...
echo ==================================================

REM Check if Node.js is installed
node --version >nul 2>&1
if errorlevel 1 (
    echo âŒ Node.js is not installed
    pause
    exit /b 1
)

echo âœ… Node.js found

REM Check if Ollama is installed
ollama --version >nul 2>&1
if errorlevel 1 (
    echo ğŸ”§ Ollama not found. Installing...
    echo.
    echo Downloading Ollama...
    powershell -Command "iwr -useb https://ollama.ai/install.sh | sh"
    
    if errorlevel 1 (
        echo âŒ Failed to install Ollama
        echo Please install manually from https://ollama.ai
        pause
        exit /b 1
    )
    
    echo âœ… Ollama installed successfully
) else (
    echo âœ… Ollama found
)

REM Start Ollama server
echo ğŸš€ Starting Ollama server...
start /B ollama serve

REM Wait for Ollama to start
echo â³ Waiting for Ollama to start...
timeout /t 10 /nobreak >nul

REM Download models
echo ğŸ“¦ Downloading LLM models...
echo.
echo Downloading Llama2 (7B model, ~4GB)...
ollama pull llama2

if errorlevel 1 (
    echo âŒ Failed to download llama2 model
    pause
    exit /b 1
)

echo âœ… Llama2 model downloaded

echo.
echo Downloading Mistral (fast, efficient model)...
ollama pull mistral

if errorlevel 1 (
    echo âš ï¸ Failed to download mistral model (optional)
) else (
    echo âœ… Mistral model downloaded
)

REM Test Ollama
echo ğŸ§ª Testing Ollama...
ollama run llama2 "Hello, can you help with poker commentary?" >nul 2>&1

if errorlevel 1 (
    echo âŒ Ollama test failed
    pause
    exit /b 1
)

echo âœ… Ollama test successful

REM Update Helm integration
echo ğŸ”§ Updating Helm integration...
echo.
echo Updating Helm engine to use local LLM...

REM Create backup of original file
if exist "helm-local-engine.js" (
    copy "helm-local-engine.js" "helm-local-engine.js.backup" >nul
    echo âœ… Backed up original engine
)

REM Copy the local LLM engine
copy "helm-local-llm-engine.js" "helm-local-engine.js" >nul
echo âœ… Updated Helm engine for local LLM

REM Update integration file
if exist "helm-integration.js" (
    copy "helm-integration.js" "helm-integration.js.backup" >nul
    echo âœ… Backed up integration file
)

echo.
echo ğŸ¯ Testing Helm with Local LLM...

REM Test the integration
node -e "
const HelmLocalLLMEngine = require('./helm-local-llm-engine');
const engine = new HelmLocalLLMEngine();
engine.initialize().then(() => {
  console.log('âœ… Helm + Local LLM integration successful!');
  process.exit(0);
}).catch((error) => {
  console.error('âŒ Integration failed:', error.message);
  process.exit(1);
});
"

if errorlevel 1 (
    echo âŒ Helm integration test failed
    pause
    exit /b 1
)

echo.
echo ==================================================
echo ğŸ‰ LOCAL LLM SETUP COMPLETE!
echo.
echo What you now have:
echo âœ… Ollama server running locally
echo âœ… Llama2 model downloaded and ready
echo âœ… Helm Control integrated with local LLM
echo âœ… Advanced AI capabilities with 100% privacy
echo.
echo Next steps:
echo 1. Start your poker game: npm run dev
echo 2. Launch Windows App: cd helm-windows-app && BUILD_AND_RUN.bat
echo 3. Test AI skills in the Windows app
echo.
echo Available AI skills:
echo - poker_commentary: AI-powered poker commentary
echo - chat_response: Advanced chat responses
echo - game_analysis: AI game state analysis
echo - player_assist: AI player assistance
echo.
echo ğŸ’° Total Cost: $0 (completely free!)
echo ğŸ”’ Privacy: 100% local, no data leaves your system
echo ğŸ§  Intelligence: Advanced LLM capabilities
echo.
echo Ollama server is running in the background.
echo To stop it later, close the Ollama window or run: taskkill /f /im ollama.exe
echo ==================================================

pause
